{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "N-gram Novelty_Evaluation_PubMed-50k_ICD-11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcUFNkHuN-F7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "937ca81d-3f1d-4fb3-cbda-903125d64a65"
      },
      "source": [
        "# Run in python console\n",
        "import nltk; nltk.download('stopwords')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5ZiHokz_c4w",
        "outputId": "6c3ed64a-9de9-482a-c0b0-3c0342838dc4"
      },
      "source": [
        "%cd drive/My\\ Drive/Colab\\ Notebooks/apex-codes/entity_sum"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/apex-codes/entity_sum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRmPOi71aZIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb7ebec-98af-45c7-8cc4-dfa00f9adb07"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import pickle as pk\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZvwjL8qHb83"
      },
      "source": [
        "## Compute Novelty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZG5yw0WaKCL",
        "outputId": "4c4a889b-b432-47e7-84b4-0ecbbba180b4"
      },
      "source": [
        "!python3 -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1hWKRq_MFDm"
      },
      "source": [
        "import sys\n",
        "import re\n",
        "import os\n",
        "import csv\n",
        "import string\n",
        "import json\n",
        "import spacy\n",
        "import nltk\n",
        "import gensim\n",
        "import collections\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "warnings.filterwarnings('ignore')\n",
        "from gensim.models import Phrases\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "#%matplotlib inline\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\")) \n",
        "stopwords = str(stop_words)\n",
        "stop = open('terrier-stop.txt')\n",
        "stopString = stop.read()\n",
        "common_terms = stopString.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4hQcKXAXV6x"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elMIfXuxXTpA"
      },
      "source": [
        "# Utility functions\n",
        "\n",
        "def preprocess_data(data):\n",
        "  data = data\n",
        "  # Remove Emails\n",
        "  data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "  # Remove new line characters\n",
        "  data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "  # Remove distracting single quotes\n",
        "  data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "\n",
        "  return data\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "######################################################################################\n",
        "\n",
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts, bigram_mod):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts, bigram_mod, trigram_mod):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    #!python3 -m spacy download en\n",
        "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out\n",
        "\n",
        "#####################################################################################\n",
        "\n",
        "def nlp_process(data_words, bigram_mod, trigram_mod):\n",
        "  # Remove Stop Words\n",
        "  data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "  # Form Bigrams\n",
        "  data_words_bigrams = make_bigrams(data_words_nostops, bigram_mod)\n",
        "\n",
        "  # Do lemmatization keeping only noun, adj, vb, adv\n",
        "  data_lemmatized = lemmatization(data_words_bigrams)\n",
        "\n",
        "  return data_lemmatized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jANt0lXDPLk3"
      },
      "source": [
        "### Unigrams generator function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V0dW6GoL72n"
      },
      "source": [
        "# This function returns unigrams for 1) abstract or 2) the collection of topic-aware citation contexts---so it must be passed the raw abstrac\n",
        "# or the raw citation contexts---this one generates top 50 unigrams\n",
        "def _generate_unigrams(text):\n",
        "  unigrams_list = []\n",
        "\n",
        "  words = nltk.word_tokenize(text)\n",
        "  fileList = [word.lower() for word in words if word.isalpha()]\n",
        "  freeList = [t for t in fileList if t not in common_terms and t not in stop_words and t not in string.punctuation]\n",
        "  freeString = [t for t in fileList if t not in common_terms and t not in stop_words and t not in string.punctuation]\n",
        "  a = ' '.join(freeString)       #conversion into a string\n",
        "  wordcount = {}\n",
        "  for word in freeList:      \n",
        "      if word not in stopwords:\n",
        "          if word not in wordcount:\n",
        "              wordcount[word] = 1\n",
        "          else:\n",
        "              wordcount[word] += 1\n",
        "  word_counter = collections.Counter(wordcount)\n",
        "  # Just grab the 50 most frequently occuring unigrams---number can change accordingly\n",
        "  for word, count in word_counter.most_common(50):\n",
        "      unigrams_list.append(word)\n",
        "\n",
        "  return unigrams_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM6ru1BOPP2u"
      },
      "source": [
        "### Bigrams and trigrams generator function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6UMjol9PR7P"
      },
      "source": [
        "def _generate_bigrams_trigrams(text):\n",
        "  #bigrams_list = []\n",
        "  # tokenize the text into individual sentences that are stored in a list\n",
        "  filtered_data = sent_tokenize(text)\n",
        "\n",
        "  data = preprocess_data(filtered_data)\n",
        "\n",
        "  #######################################################################################\n",
        "  data_words = list(sent_to_words(data))\n",
        "\n",
        "\n",
        "  # Build the bigram and trigram models\n",
        "  bigram = gensim.models.Phrases(data_words, min_count=1, threshold=1) # higher threshold fewer phrases.\n",
        "  trigram = gensim.models.Phrases(bigram[data_words], min_count=1, threshold=1)  \n",
        "\n",
        "  # Faster way to get a sentence clubbed as a trigram/bigram\n",
        "  bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "  trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "  data_lemmatized = nlp_process(data_words, bigram_mod, trigram_mod)\n",
        "\n",
        "  sentence_stream = data_lemmatized\n",
        "\n",
        "  bigrams_lemmatized = []\n",
        "  trigrams_lemmatized = []\n",
        "\n",
        "  bigram = Phrases(sentence_stream, min_count=3, delimiter=b'_')\n",
        "  trigram  = Phrases(bigram[sentence_stream], min_count=3, delimiter=b'_')\n",
        "  for sent in sentence_stream:\n",
        "      bigrams_ = [b for b in bigram[sent] if b.count('_') == 1]\n",
        "      trigrams_ = [t for t in trigram[bigram[sent]] \n",
        "                                            if t.count('_')==2]\n",
        "      bigrams_lemmatized.append(bigrams_)\n",
        "      trigrams_lemmatized.append(trigrams_)\n",
        "  \n",
        "  ngrams_lemmatized =  trigrams_lemmatized + bigrams_lemmatized\n",
        "\n",
        "  return bigrams_lemmatized, trigrams_lemmatized\n",
        "  #return ngrams_lemmatized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEj3bMUON2v0"
      },
      "source": [
        "# Compute N-gram Novelty with reference to source articles\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jWqUvQUNP3V"
      },
      "source": [
        "def _compute_novelty_I(gen_summary, human_summary):\n",
        "  # generate unigrams for the summary and the abstract\n",
        "  gen_summary_unigrams = _generate_unigrams(gen_summary)\n",
        "  human_summary_unigrams = _generate_unigrams(human_summary)\n",
        "\n",
        "  # generate bigrams and trigrams for the summary and the abstract\n",
        "  gen_summary_bi_trigrams, gen_summary_tri_trigrams = _generate_bigrams_trigrams(gen_summary)\n",
        "  human_summary_bi_trigrams, human_summary_tri_trigrams = _generate_bigrams_trigrams(human_summary)\n",
        "\n",
        "  # Total n-grams for the summary and the abstract\n",
        "  gen_summary_ngrams = gen_summary_unigrams + gen_summary_bi_trigrams + gen_summary_tri_trigrams\n",
        "  human_summary_ngrams = human_summary_unigrams + human_summary_bi_trigrams + human_summary_tri_trigrams\n",
        "\n",
        "\n",
        "  diff_list = [ngram for ngram in gen_summary_ngrams if ngram not in human_summary_ngrams]   # unigram that is in a summary but not in the abstract\n",
        "\n",
        "  novelty_score = float(len(diff_list)) / float(len(gen_summary_ngrams))\n",
        "  novelty_score = novelty_score * 100\n",
        "\n",
        "  return novelty_score\n",
        "\n",
        "  #print(f\"{RESULTS_PATH} Novelty Score wrt abstracts: %.2f\" % novelty_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjs2dzqaA2-W"
      },
      "source": [
        "## DON'T DO Novelty wrt human summary - Evaluation against ground truth summaries (Yet, don't do this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S254gwTvrWEP"
      },
      "source": [
        "## Evaluation against the source articles themselves"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  lst_modelName = [\"T5\", \"BART\", \"Pegasus\"]\n",
        "  lst_model_type = [\"vanilla\", \"w_named_entities\"]\n",
        "\n",
        "  dict_novelty_scores = defaultdict(list)\n",
        "\n",
        "  for modelName in lst_modelName:\n",
        "    for model_type in lst_model_type:\n",
        "      SUMMARY_PATH = f\"pubmed-FINAL-SUMMARIES/{modelName}\"\n",
        "      input_filename_path = f\"{SUMMARY_PATH}/pubmed-summaries-{model_type}.jsonl\"\n",
        "\n",
        "      output_results_path = f\"FINAL_RESULTS/NOVELTY\"\n",
        "      os.makedirs(output_results_path, exist_ok=True)\n",
        "      output_file_name = \"n_gram_novelty_scores.csv\"\n",
        "\n",
        "      novelty_sum = 0.0   # initialize all cumulative scores to zero\n",
        "      total_no_summaries = 0\n",
        "      with open(input_filename_path) as fp:\n",
        "        for iter, line in enumerate(fp):\n",
        "          if iter % 2000 == 0:\n",
        "            print(\"Iteration: \", iter)\n",
        "          dict_data = json.loads(line)\n",
        "\n",
        "          generated_summary = dict_data[\"abstractive_summary\"]    # generated summary\n",
        "          #ground_truth_summary = dict_data[\"article_abstract\"]   # human-like summary\n",
        "          article_text = dict_data[\"article_text\"]   # source input article----to do evaluation wrt the input article\n",
        "\n",
        "          # call to the ROUGE computing method\n",
        "          try:\n",
        "            novelty_score = _compute_novelty_I(generated_summary, article_text)\n",
        "\n",
        "            novelty_sum += novelty_score\n",
        "            \n",
        "            total_no_summaries += 1\n",
        "\n",
        "          except:\n",
        "            continue\n",
        "\n",
        "      fp.close()\n",
        "\n",
        "      avg_novelty = novelty_sum/float(total_no_summaries)\n",
        "\n",
        "      dict_novelty_scores[\"training-config\"].append(f\"{modelName}-{model_type}\")\n",
        "      dict_novelty_scores[\"n-gram-novelty\"].append(avg_novelty)\n",
        "\n",
        "  df_novelty_scores = pd.DataFrame(dict_novelty_scores)\n",
        "\n",
        "  print(df_novelty_scores)\n",
        "\n",
        "  df_novelty_scores.to_excel(\"n_gram_novelty_scores.xlsx\")  "
      ],
      "metadata": {
        "id": "WUVhcgY-JdIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_XAOELKheRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50fe6b1f-812f-44ce-98a1-175e1a8d3c0e"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0\n",
            "Iteration:  2000\n",
            "Iteration:  4000\n",
            "Iteration:  0\n",
            "Iteration:  2000\n",
            "Iteration:  4000\n",
            "Iteration:  0\n",
            "Iteration:  2000\n",
            "Iteration:  4000\n",
            "Iteration:  0\n",
            "Iteration:  2000\n",
            "Iteration:  4000\n",
            "Iteration:  0\n",
            "Iteration:  2000\n",
            "Iteration:  4000\n",
            "Iteration:  0\n",
            "Iteration:  2000\n",
            "Iteration:  4000\n",
            "            training-config  n-gram-novelty\n",
            "0                T5-vanilla       52.930007\n",
            "1       T5-w_named_entities       50.079039\n",
            "2              BART-vanilla       54.816223\n",
            "3     BART-w_named_entities       54.959429\n",
            "4           Pegasus-vanilla       51.260270\n",
            "5  Pegasus-w_named_entities       52.558129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEQC5XmGsdPy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCUhQWuCsdHc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}