{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LS0BC5oErcAP",
    "outputId": "7d9fdc1c-0061-49a5-8b33-7350126ac4a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sT8qQNCSreSW",
    "outputId": "d2a69c15-1ce4-468f-c704-51ca8a0b54f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/apex-codes/entity_sum\n"
     ]
    }
   ],
   "source": [
    "%cd drive/My\\ Drive/Colab\\ Notebooks/apex-codes/entity_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E5qKrgfQYmVg",
    "outputId": "52b7fa86-c5ff-4a56-a933-12d7fecbb139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |███████▍                        | 10 kB 35.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 20 kB 39.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 30 kB 21.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 40 kB 17.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 44 kB 3.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 71 kB 5.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 13.5 MB 24.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 31.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 188 kB 50.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 55.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 451 kB 55.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 628 kB 47.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 33.1 MB 79 kB/s \n",
      "\u001b[?25h  Building wheel for en-core-sci-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 125.1 MB 39 kB/s \n",
      "\u001b[?25h  Building wheel for en-ner-bc5cdr-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 516 kB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.62.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.10.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (21.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.6.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.13)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (57.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (5.2.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 2.2.5\n",
      "    Uninstalling en-core-web-sm-2.2.5:\n",
      "      Successfully uninstalled en-core-web-sm-2.2.5\n",
      "Successfully installed en-core-web-sm-3.0.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q scispacy\n",
    "!pip3 install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_sm-0.3.0.tar.gz\n",
    "!pip3 install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_bc5cdr_md-0.4.0.tar.gz\n",
    "\n",
    "!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Eq2dLdx2XSm"
   },
   "outputs": [],
   "source": [
    "!pip3 install -q jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l85klovdbwxW",
    "outputId": "90c34db3-4124-435c-c4fc-4f7873bc388a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/spacy/util.py:718: UserWarning: [W094] Model 'en_core_sci_sm' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.7,<3.1.0\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "import spacy\n",
    "import scispacy\n",
    "from spacy import displacy\n",
    "import en_core_sci_sm\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from scispacy.linking import EntityLinker\n",
    "from collections import (OrderedDict,Counter, defaultdict)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from pprint import pprint\n",
    "import json\n",
    "import jsonlines\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAhjCSVN4rdX"
   },
   "outputs": [],
   "source": [
    "def _get_named_entities(ext_sum):\n",
    "  nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "  doc = nlp(ext_sum)\n",
    "  entities = []\n",
    "  for ent in doc.ents:\n",
    "    entities.append(ent.text)\n",
    "  str_entities = \" | \".join(entities)   # a string representation of list of entities with the pipe symbol as a separator\n",
    "  \n",
    "  return str_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvOanz4G2GRe"
   },
   "source": [
    "## Read the json lines files containing the pubmed article texts and abstracts, generate named entities from the article_text and the abstract as well after parsing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9oOokor2MG1"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  fileName = \"train_data_50k.jsonl\"\n",
    "  DATA_PATH = \"pubmed_dataset/pubmed-dataset\"\n",
    "\n",
    "  with open(f\"{DATA_PATH}/{fileName}\") as f:\n",
    "    for idx, line in enumerate(f):\n",
    "      if idx % 1000 == 0:\n",
    "        print(\"Iteration: \", idx)\n",
    "\n",
    "      data = json.loads(line)\n",
    "      if idx > 2.5e4:\n",
    "        break\n",
    "      \n",
    "      article_text = \" \".join(data['article_text'])  ##\n",
    "      article_abstract = \" \".join(data['abstract_text']).replace('<S>', '').replace('</S>', '')   ##\n",
    "\n",
    "      article_text_named_entities = _get_named_entities(article_text)\n",
    "      article_abstract_named_entities = _get_named_entities(article_abstract)\n",
    "\n",
    "      dict_1 = {\"article_text\" : article_text,\n",
    "                \"article_abstract\" : article_abstract,\n",
    "                \"article_text_named_entities\" : article_text_named_entities,\n",
    "                \"article_abstract_named_entities\" : article_abstract_named_entities\n",
    "                }\n",
    "        \n",
    "      with jsonlines.open(\"pubmed_w_named_entities_25k.jsonl\", \"a\") as writer:\n",
    "        writer.write(dict_1)\n",
    "      writer.close()\n",
    "\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsvjPwUev3vT"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWcWhDCPw6JD"
   },
   "source": [
    "## Read back a sample object from the jsonlines file saved above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSnofcFcxfS6",
    "outputId": "2b899d90-868b-4243-98cc-e71607c06da8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/apex-codes/entity_sum\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6-hPR5yxDi1"
   },
   "outputs": [],
   "source": [
    "fileName = \"pubmed_w_named_entities_25k.jsonl\"\n",
    "lst_data = list()\n",
    "\n",
    "with open(f\"{fileName}\") as f:\n",
    "  for idx, line in enumerate(f):\n",
    "    if idx == 20:\n",
    "      break\n",
    "    data = json.loads(line)\n",
    "    lst_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_khPkKe-3g1k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0az5wCFkyVwW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI1Ur2cYRMaY"
   },
   "source": [
    "## https://towardsdatascience.com/understanding-faiss-619bb6db2d1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATGadjcSPDep",
    "outputId": "abdabd21-360c-4421-dbb4-24570f800c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-gpu\n",
      "  Downloading faiss_gpu-1.7.1.post3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (90.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 90.1 MB 21 kB/s \n",
      "\u001b[?25hInstalling collected packages: faiss-gpu\n",
      "Successfully installed faiss-gpu-1.7.1.post3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AenW7NVcPy3J"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import faiss  # this will import the faiss library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4lGHZWTB9Jf"
   },
   "outputs": [],
   "source": [
    "res = faiss.StandardGpuResources()  # use a single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByXslqTPB-fQ",
    "outputId": "a5ed80c7-13a3-4464-8c1d-d1ed3be01ce7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.StandardGpuResources; proxy of <Swig Object of type 'faiss::gpu::StandardGpuResources *' at 0x7f0dce6f0e10> >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYU5D59dQx0C"
   },
   "outputs": [],
   "source": [
    "dimension = 128    # dimensions of each vector                         \n",
    "n = 200    # number of vectors                   \n",
    "np.random.seed(1)             \n",
    "db_vectors = np.random.random((n, dimension)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-YZDpUS6Qxb4",
    "outputId": "ba7efb48-aef9-4200-fddc-ede60e00ed6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 128)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAi-bhZLQxX-"
   },
   "outputs": [],
   "source": [
    "nlist = 5  # number of clusters\n",
    "quantiser = faiss.IndexFlatL2(dimension)  \n",
    "index = faiss.IndexIVFFlat(quantiser, dimension, nlist,   faiss.METRIC_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FAYYpo0rQxTK",
    "outputId": "80dea978-4677-4dc7-b3c7-f0f93143c346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n",
      "True\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(index.is_trained)   # False\n",
    "index.train(db_vectors)  # train on the database vectors\n",
    "print(index.ntotal)   # 0\n",
    "index.add(db_vectors)   # add the vectors and update the index\n",
    "print(index.is_trained)  # True\n",
    "print(index.ntotal)   # 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdNsvwOXRQz6"
   },
   "outputs": [],
   "source": [
    "nprobe = 2  # find 2 most similar clusters\n",
    "n_query = 10  \n",
    "k = 3  # return 3 nearest neighbours\n",
    "np.random.seed(0)   \n",
    "query_vectors = np.random.random((n_query, dimension)).astype('float32')\n",
    "distances, indices = index.search(query_vectors, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AxeCEaRQRQw3",
    "outputId": "7a7ba2f7-8f6f-4e8a-dd52-e9c127c27a15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.7704525, 16.77301  , 17.171312 ],\n",
       "       [16.476105 , 18.52229  , 18.811914 ],\n",
       "       [15.520994 , 16.50026  , 17.069548 ],\n",
       "       [16.842714 , 17.712343 , 17.828485 ],\n",
       "       [18.325394 , 18.495459 , 18.684458 ],\n",
       "       [17.531889 , 18.181791 , 18.331264 ],\n",
       "       [16.84016  , 17.036638 , 17.091757 ],\n",
       "       [15.984031 , 16.380919 , 17.270594 ],\n",
       "       [18.018503 , 18.076103 , 18.766174 ],\n",
       "       [17.11392  , 17.385286 , 17.657572 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEVdGHjTPDZA",
    "outputId": "79118e22-e393-45a3-d2e7-b026933432d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[185,  35,  96],\n",
       "       [118,  51, 122],\n",
       "       [148, 149, 173],\n",
       "       [175, 177,  50],\n",
       "       [ 44, 144, 174],\n",
       "       [156,  74, 151],\n",
       "       [ 57, 144,  18],\n",
       "       [ 82,  12,  46],\n",
       "       [ 52,  73,  59],\n",
       "       [ 82,  46,  90]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4PRxisMS7kI"
   },
   "outputs": [],
   "source": [
    "# the dict_data passed here is a json object with article_text, article_abstrat, article_text_named_entities and\n",
    "# article_abstract_named_entities\n",
    "# K - number of facts per named entity to be summarized (here the facts are retrieved such that they should be semantically similar to the \n",
    "# sentences where the named entity appears in)\n",
    "'''\n",
    "import itertools\n",
    "\n",
    "res = faiss.StandardGpuResources()  # use a single GPU\n",
    "\n",
    "\n",
    "def _get_entity_based_facts(dict_data, top_k=3):\n",
    "  dict_knowledge_facts_final = defaultdict(list)   # to store facts extracted from the background knowledge bases (UMLS, ICD-11, SNOMED-CT)\n",
    "  lst_knowledge_facts_total = list()\n",
    "\n",
    "  lst_named_entities_text = list(set(dict_data['article_text_named_entities'].split(' | ')))\n",
    "  \n",
    "  input_text = dict_data['article_text']   # the input_text to be summarized into the abstract---this is what is to be tokenized\n",
    "  \n",
    "  input_text_sentences = sent_tokenize(input_text) # this gives us a list of sentences\n",
    "\n",
    "  lst_query_embeddings = []   # This is for storing avg sentene embeddings of all sentences corresponding to a named entity\n",
    "  lst_fact_embeddings = []    # to build the faiss index\n",
    "  for named_entity in lst_named_entities_text:\n",
    "    sentences_w_named_entity = [sent for sent in input_text_sentences if named_entity in sent]  # Get sentences with the named entity\n",
    "  \n",
    "    try:\n",
    "      facts_complete = PYM.search(named_entity)\n",
    "\n",
    "      #lst_knowledge_facts = list()\n",
    "      for fact_str in facts_complete:        \n",
    "        fact = str(fact_str).split(' # ')[1].strip()   # the actual description linking the named entity pair (disregarding the CUI identifier)\n",
    "        lst_knowledge_facts_total.append(fact)\n",
    "    except:\n",
    "      continue\n",
    "\n",
    "    sentence_w_named_entities_embeddings = [biobert.sentence_vector(sent).numpy() for sent in sentences_w_named_entity]\n",
    "    # avg embedding of all sentences where the named entity appears in\n",
    "    avg_sentence_w_entities_embedding = np.mean(np.array(sentence_w_named_entities_embeddings), axis=0)   # to be used as a query\n",
    "    \n",
    "    lst_query_embeddings.append(avg_sentence_w_entities_embedding)   # list of all sentences embeddings for a document (this could be a numpy matrix)\n",
    "    \n",
    "\n",
    "  for fact in lst_knowledge_facts_total:\n",
    "    fact_biobert_embedding = biobert.sentence_vector(fact)\n",
    "    lst_fact_embeddings.append(fact_biobert_embedding)\n",
    "\n",
    "  np_query_embeddings = np.asarray(lst_query_embeddings)\n",
    "  np_fact_embeddings = np.asarray(lst_fact_embeddings)\n",
    "\n",
    "  d = np_query_embeddings.shape[1]   # grab the vector dimension\n",
    "  #print(d)\n",
    "  #raise KeyboardInterrupt\n",
    "\n",
    "  quantiser = faiss.IndexFlatL2(d)  \n",
    "  index = faiss.IndexIVFFlat(quantiser, d, faiss.METRIC_L2)\n",
    "  # make it an IVF GPU index\n",
    "  index = faiss.index_cpu_to_gpu(res, 0, index)  # added on 1129\n",
    "\n",
    "  index.train(np_fact_embeddings)  # train on the database vectors\n",
    "  #print(index.ntotal)   # 0\n",
    "  index.add(np_fact_embeddings)   # add the vectors and update the index\n",
    "  #######################################################\n",
    "  '''\n",
    "  index = faiss.IndexFlatL2(d)   # build the index\n",
    "  #print(index.is_trained)\n",
    "  index.add(np_fact_embeddings)                  # add knowledge (fact) vectors to the index\n",
    "  #print(index.ntotal)\n",
    "  '''\n",
    "\n",
    "  #k = 4                          # we want to see 4 nearest neighbors\n",
    "  distances, indices = index.search(np_query_embeddings, top_k)     # actual search, I is the index\n",
    "  #print(indices[:5])                   # neighbors of the 5 first queries\n",
    "  #print(indices[-5:])                  # neighbors of the 5 last queries\n",
    "\n",
    "  total_indices = list(itertools.chain(*indices))\n",
    "  final_facts = list(set([lst_knowledge_facts_total[idx] for idx in total_indices]))\n",
    "  knowledge_facts_total_str = \" | \".join(final_facts)  # Join all the retrieved facts with the pipe separator\n",
    "\n",
    "  return knowledge_facts_total_str\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cljedeE6S7cK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VgIMN29S7Vm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVUhg_D8D0XJ"
   },
   "source": [
    "## Now, onto the actual knowledge retriever (from UMLS etc) component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cK8p8xqLUFpa",
    "outputId": "8930f654-e4eb-4337-c988-e7b45027f05f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting owlready2\n",
      "  Downloading Owlready2-0.35.tar.gz (23.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.8 MB 1.3 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: owlready2\n",
      "  Building wheel for owlready2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for owlready2: filename=Owlready2-0.35-cp37-cp37m-linux_x86_64.whl size=20432789 sha256=0edc01938b7996ea13fd612b5b61d778b3ec30d7db3f323f79d6f1f6d9903d4c\n",
      "  Stored in directory: /root/.cache/pip/wheels/a6/85/8f/4f254dc1d3b7901c23004544f7214748607d8b0c2c02b3c868\n",
      "Successfully built owlready2\n",
      "Installing collected packages: owlready2\n",
      "Successfully installed owlready2-0.35\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install -q Owlready2k\n",
    "!pip3 install owlready2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WofvL4Yhh1gD",
    "outputId": "0e41bcaa-1142-446e-ffad-b93a021dcdc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 3.3 MB 6.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 895 kB 57.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 63.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 61 kB 568 kB/s \n",
      "\u001b[K     |████████████████████████████████| 596 kB 58.9 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip3 install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCr_X840hySY"
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtNRq4Bedx6X",
    "outputId": "ab4e8d27-a36f-47cf-f226-375cc82af2b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 748.9 MB 661 bytes/s \n",
      "\u001b[K     |████████████████████████████████| 123 kB 65.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 131 kB 62.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 8.4 MB 53.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 79 kB 9.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 138 kB 75.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 127 kB 68.1 MB/s \n",
      "\u001b[?25h  Building wheel for biobert-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.2.0 which is incompatible.\n",
      "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.2.0 which is incompatible.\n",
      "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.2.0 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q biobert-embedding==0.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18FXRE0mhjgz"
   },
   "outputs": [],
   "source": [
    "from biobert_embedding.embedding import BiobertEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsGkLRsWhp5b"
   },
   "outputs": [],
   "source": [
    "biobert = BiobertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNPxzWuIUFgq"
   },
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "from owlready2.pymedtermino2 import *\n",
    "from owlready2.pymedtermino2.umls import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDr0BcMwqKzK",
    "outputId": "bc9a0967-2b2c-4ade-92e3-8321c388a47d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing UMLS from umls-2021AA-full.zip with Python version 3.7.12 and Owlready version 2-0.35...\n",
      "Full UMLS release - importing UMLS from inner Zip file 2021AA-full/2021aa-1-meta.nlm...\n",
      "  Parsing 2021AA/META/MRSTY.RRF.gz as MRSTY with encoding UTF-8\n",
      "  Parsing 2021AA/META/MRRANK.RRF.gz as MRRANK with encoding UTF-8\n",
      "  Parsing 2021AA/META/MRCONSO.RRF.aa.gz as MRCONSO with encoding UTF-8\n",
      "  Parsing 2021AA/META/MRCONSO.RRF.ab.gz as MRCONSO with encoding UTF-8\n",
      "  Parsing 2021AA/META/MRDEF.RRF.gz as MRDEF with encoding UTF-8\n",
      "Full UMLS release - importing UMLS from inner Zip file 2021AA-full/2021aa-2-meta.nlm...\n",
      "  Parsing 2021AA/META/MRREL.RRF.aa.gz as MRREL with encoding UTF-8\n",
      "  Parsing 2021AA/META/MRREL.RRF.ab.gz as MRREL with encoding UTF-8\n",
      "  Parsing 2021AA/META/MRREL.RRF.ac.gz as MRREL with encoding UTF-8\n",
      "  Parsing 2021AA/META/MRREL.RRF.ad.gz as MRREL with encoding UTF-8\n",
      "  Parsing 2021AA/META/MRSAT.RRF.aa.gz as MRSAT with encoding UTF-8\n",
      "  Parsing 2021AA/META/MRSAT.RRF.ab.gz as MRSAT with encoding UTF-8\n",
      "  Parsing 2021AA/META/MRSAT.RRF.ac.gz as MRSAT with encoding UTF-8\n",
      "  Parsing 2021AA/META/MRSAT.RRF.ad.gz as MRSAT with encoding UTF-8\n",
      "  Parsing 2021AA/META/MRSAT.RRF.ae.gz as MRSAT with encoding UTF-8\n",
      "Breaking ORIG cycles...\n",
      "    SNOMEDCT_US : 0 cycles found: \n",
      "    ICD10 : 0 cycles found: \n",
      "    SRC : 0 cycles found: \n",
      "Finalizing only properties and restrictions...\n",
      "Finalizing CUI - ORIG mapping...\n",
      "FTS Indexing...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "get_ontology(\"http://PYM/\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#default_world.set_backend(filename = \"pym.sqlite3\")\n",
    "import_umls(\"umls-2021AA-full.zip\", terminologies = [\"ICD10\", \"SNOMEDCT_US\", \"CUI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAzs5WrMUFWE"
   },
   "outputs": [],
   "source": [
    "PYM = get_ontology(\"http://PYM/\").load()\n",
    "default_world.save()\n",
    "CUI = PYM[\"CUI\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gm_1yVEHUbee"
   },
   "source": [
    "### Method to retrieve top-K facts from background KB for each sentence having the named entities extracted. First, tokenize the input_text into sentences and for each named entity, grab the top-K facts and measure semantic similarity using FAISS with the sentence in which the named entity appears in. Ablation study to be conducted with different values of K (5, 10, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fP1-m7Yqv8l8"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80U0nC2LUFRa"
   },
   "outputs": [],
   "source": [
    "# the dict_data passed here is a json object with article_text, article_abstrat, article_text_named_entities and\n",
    "# article_abstract_named_entities\n",
    "# K - number of facts per named entity to be summarized (here the facts are retrieved such that they should be semantically similar to the \n",
    "# sentences where the named entity appears in)\n",
    "\n",
    "\n",
    "def _get_entity_based_facts(dict_data, top_k=5):\n",
    "  dict_knowledge_facts_final = defaultdict(list)   # to store facts extracted from the background knowledge bases (UMLS, ICD-11, SNOMED-CT)\n",
    "  lst_knowledge_facts_total = list()\n",
    "\n",
    "  lst_named_entities_text = list(set(dict_data['article_text_named_entities'].split(' | ')))\n",
    "  \n",
    "  input_text = dict_data['article_text']   # the input_text to be summarized into the abstract---this is what is to be tokenized\n",
    "  \n",
    "  input_text_sentences = sent_tokenize(input_text) # this gives us a list of sentences\n",
    "  for named_entity in lst_named_entities_text:\n",
    "    sentences_w_named_entity = [sent for sent in input_text_sentences if named_entity in sent]  # Get sentences with the named entity\n",
    "  \n",
    "    try:\n",
    "      facts_complete = PYM.search(named_entity)\n",
    "\n",
    "      lst_knowledge_facts = list()\n",
    "      for fact_str in facts_complete:        \n",
    "        fact = str(fact_str).split(' # ')[1].strip()   # the actual description linking the named entity pair (disregarding the CUI identifier)\n",
    "        lst_knowledge_facts.append(fact)\n",
    "    except:\n",
    "      continue\n",
    "\n",
    "    sentence_w_named_entities_embeddings = [biobert.sentence_vector(sent).numpy() for sent in sentences_w_named_entity]\n",
    "    # avg embedding of all sentences where the named entity appears in\n",
    "    avg_sentence_w_entities_embedding = np.mean(np.array(sentence_w_named_entities_embeddings), axis=0)   # to be used as a query\n",
    "\n",
    "    # grab top-k facts with high cosine similarity with the avg_sentence_embedding of all sentencew where the named entity appears in\n",
    "    dict_fact_cosine = {}\n",
    "    for fact in lst_knowledge_facts:\n",
    "      fact_biobert_embedding = biobert.sentence_vector(fact)\n",
    "      cosine_sim = 1 - distance.cosine(fact_biobert_embedding, avg_sentence_w_entities_embedding)   # cosine distance between fact embedding and avg sent\n",
    "      dict_fact_cosine[fact] = cosine_sim\n",
    "\n",
    "    # sort the facts based on cosine_sim and return the top-k facts\n",
    "    top_k_facts = list({k: v for k, v in sorted(dict_fact_cosine.items(), \n",
    "                                                key=lambda item: item[1], reverse=True)}.keys())[:top_k]\n",
    "\n",
    "\n",
    "    lst_knowledge_facts_total += top_k_facts   # This is for the entire input_text\n",
    "\n",
    "  knowledge_facts_total_str = \" | \".join(lst_knowledge_facts_total)  # Join all the facts with the pipe separator\n",
    "\n",
    "\n",
    "  return knowledge_facts_total_str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkd3AnDCUNkd"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  input_fileName = \"pubmed_w_named_entities_25k.jsonl\"\n",
    "  output_fileName = \"pubmed_w_named_entities_knowledge_facts_25k.jsonl\"\n",
    "\n",
    "  with open(f\"{input_fileName}\") as f:\n",
    "    for idx, line in enumerate(f):\n",
    "      if idx > 1524:\n",
    "        try: \n",
    "          if idx % 100 == 0:\n",
    "            print(\"Iteration: \", idx)\n",
    "          dict_data = json.loads(line)\n",
    "          article_text_facts = _get_entity_based_facts(dict_data)   # call to the knowledge (fact) retriever method\n",
    "\n",
    "\n",
    "          dict_1 = {\"article_text\" : dict_data['article_text'],\n",
    "                    \"article_abstract\" : dict_data['article_abstract'],\n",
    "                    \"article_text_named_entities\" : dict_data['article_text_named_entities'],\n",
    "                    \"article_abstract_named_entities\" : dict_data['article_abstract_named_entities'],\n",
    "                    \"article_text_facts\" : article_text_facts\n",
    "                    }\n",
    "              \n",
    "          with jsonlines.open(output_fileName, \"a\") as writer:\n",
    "            writer.write(dict_1)\n",
    "          writer.close()\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yiPiesb7UNgL",
    "outputId": "0262f00f-adec-4ddf-a11e-9f61757118d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1600\n",
      "Iteration:  1700\n",
      "Iteration:  1800\n",
      "Iteration:  1900\n",
      "Iteration:  2000\n",
      "Iteration:  2100\n",
      "Iteration:  2200\n",
      "Iteration:  2300\n",
      "Iteration:  2400\n",
      "Iteration:  2500\n",
      "Iteration:  2600\n",
      "Iteration:  2700\n",
      "Iteration:  2800\n",
      "Iteration:  2900\n",
      "Iteration:  3000\n",
      "Iteration:  3100\n",
      "Iteration:  3200\n",
      "Iteration:  3300\n",
      "Iteration:  3400\n",
      "Iteration:  3500\n",
      "Iteration:  3600\n",
      "Iteration:  3700\n",
      "Iteration:  3800\n",
      "Iteration:  3900\n",
      "Iteration:  4000\n",
      "Iteration:  4100\n",
      "Iteration:  4200\n",
      "Iteration:  4300\n",
      "Iteration:  4400\n",
      "Iteration:  4500\n",
      "Iteration:  4600\n",
      "Iteration:  4700\n",
      "Iteration:  4800\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCVOMeKDUWzn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5vqgk0tUWnX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OgbSa1pUWiO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "knowledge_retriever_pubmed.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
