{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Pubmed_Abstracts_Parser.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNVWhCPbOmUn",
        "outputId": "8888b0f3-f021-4850-8da0-86aded66e310"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv5m6sp3OnFd",
        "outputId": "d8127e59-94d5-4c75-91fd-d36e6438be4d"
      },
      "source": [
        "%cd drive/My\\ Drive/Colab\\ Notebooks/apex-codes/entity_sum"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/apex-codes/entity_sum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6PHO_quOw6M",
        "outputId": "b0f63ec6-d02a-40e2-835e-d1fb33d6cd5e"
      },
      "source": [
        "!pip3 install Bio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Bio\n",
            "  Downloading bio-1.3.3-py3-none-any.whl (271 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 271 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting mygene\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Collecting biopython>=1.79\n",
            "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 40.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from Bio) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from Bio) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython>=1.79->Bio) (1.19.5)\n",
            "Collecting biothings-client>=0.2.6\n",
            "  Downloading biothings_client-0.2.6-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (1.24.3)\n",
            "Installing collected packages: biothings-client, mygene, biopython, Bio\n",
            "Successfully installed Bio-1.3.3 biopython-1.79 biothings-client-0.2.6 mygene-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tArgnTaPPo0",
        "outputId": "c1b23cb4-3c6c-428e-af28-7ee48665624a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ggYNtnG54Hb"
      },
      "source": [
        "## Import Python Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlm5yvLQ54Hc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "outputId": "b9b84964-8073-4084-8520-d166c09284d1"
      },
      "source": [
        "import io\n",
        "from Bio import Entrez, Medline\n",
        "import pandas as pd\n",
        "from pandas import ExcelWriter\n",
        "from pandas import ExcelFile\n",
        "import re\n",
        "import collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from gensim.models import Doc2Vec\n",
        "from sklearn import utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,make_scorer,precision_score,recall_score,roc_auc_score,f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import urllib.request, urllib.error, urllib.parse\n",
        "import json\n",
        "import os\n",
        "from pprint import pprint\n",
        "import nltk\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "from bs4 import BeautifulSoup\n",
        "import plotly.graph_objs as go\n",
        "import cufflinks\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import plotly.figure_factory as ff\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "from plotly.offline import iplot\n",
        "cufflinks.go_offline()\n",
        "cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
        "from bs4 import BeautifulSoup\n",
        "from rouge import Rouge\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBIv6pMx54Hd"
      },
      "source": [
        "## Data Collection & Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEchbo2m54He"
      },
      "source": [
        "## Parse medline "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNTIyNsF54He"
      },
      "source": [
        "def search_medline(query):\n",
        "  Entrez.email = \"aman.fanta@gmail.com\"\n",
        "  search = Entrez.esearch(db='pubmed', term=query, retmax = 1000, usehistory='y', reldate=90)\n",
        "  handle = Entrez.read(search)\n",
        "\n",
        "  try:\n",
        "      return handle\n",
        "  except Exception as e:\n",
        "      raise IOError(str(e))\n",
        "  finally:\n",
        "      search.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoVOpAE-54He"
      },
      "source": [
        "def fetch_rec(rec_id, entrez_handle):\n",
        "    fetch_handle = Entrez.efetch(db='pubmed', id = rec_id, rettype='Medline', retmode='text',\n",
        "                                 webenv=entrez_handle['WebEnv'],query_key=entrez_handle['QueryKey'], reldate=90)\n",
        "    rec = fetch_handle.read()\n",
        "    \n",
        "    return rec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDKpYZqJUazu"
      },
      "source": [
        "def _get_search_query_terms(LEXICON_DIR_PATH, icd_chapter_name):\n",
        "  with open(f\"{LEXICON_DIR_PATH}/{icd_chapter_name}.csv\", 'r') as fp:\n",
        "    disease_list = fp.read().splitlines()\n",
        "    disease_list = disease_list[:500]   # the first 500 query terms\n",
        "\n",
        "    query = \"\"\n",
        "\n",
        "    # For complete string of query terms Bio Entrez parser expects. \"or\" is used as the delimiter between two query terms\n",
        "    for disease in disease_list:\n",
        "      query = (query) + \" or \" + (disease)\n",
        "\n",
        "  fp.close()\n",
        "\n",
        "  return query\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "2sdjwxCv54Hf"
      },
      "source": [
        "def _query_parse_medline_abstracts(icd_chapter_name, complete_query_terms) :\n",
        "  count = 0\n",
        "  fau = \"\"\n",
        "  mh = \"\"\n",
        "  j = 0\n",
        "  rec_handler = search_medline(complete_query_terms)\n",
        "  count = int(rec_handler[\"Count\"])\n",
        "  batch_size = 200\n",
        "  \n",
        "  df = pd.DataFrame(columns = ['PMID', 'Title', 'Abstract', 'MeSH', 'PubType', 'PubDate', 'Journal', 'DOI', 'PMC'])\n",
        "          \n",
        "  for recId in rec_handler['IdList']:\n",
        "\n",
        "    if j % 100 == 0 and j != 0:\n",
        "        print(j)\n",
        "    j += 1\n",
        "\n",
        "    fau = \"\"\n",
        "    mh = \"\"\n",
        "    rec = fetch_rec(recId, rec_handler)\n",
        "    r = io.StringIO(rec)\n",
        "    medline_rec = Medline.read(r)\n",
        "\n",
        "    if 'PMID' in medline_rec:\n",
        "        df.loc[j, 'PMID'] = medline_rec['PMID']\n",
        "\n",
        "    if 'TI' in medline_rec:\n",
        "        df.loc[j, 'Title'] = medline_rec['TI']\n",
        "    else:\n",
        "        df.loc[j, 'Title'] = 'null'\n",
        "\n",
        "    if 'AB' in medline_rec:\n",
        "        df.loc[j, 'Abstract'] = medline_rec['AB']\n",
        "    else:\n",
        "        df.loc[j, 'Abstract'] = 'null'\n",
        "\n",
        "    if 'PT' in medline_rec:\n",
        "        df.loc[j, 'PubType'] = medline_rec['PT']\n",
        "    else:\n",
        "        df.loc[j, 'PubType'] = 'null'\n",
        "\n",
        "    if 'FAU' in medline_rec:\n",
        "        for y in medline_rec['FAU']:\n",
        "            fau = fau + y + ', '\n",
        "        df.loc[j, 'FAU'] = fau\n",
        "    else:\n",
        "        df.loc[j, 'FAU'] = 'null'\n",
        "\n",
        "    if 'MH' in medline_rec:\n",
        "        for x in medline_rec['MH']:\n",
        "            mh = mh + x + ', '\n",
        "        df.loc[j, 'MeSH'] = mh\n",
        "    else:\n",
        "        df.loc[j, 'MeSH'] = 'null'\n",
        "\n",
        "    if 'DP' in medline_rec:\n",
        "        df.loc[j, 'PubDate'] = medline_rec['DP']\n",
        "    else:\n",
        "        df.loc[j, 'PubDate'] = 'null'\n",
        "\n",
        "    if 'JT' in medline_rec:\n",
        "        df.loc[j, 'Journal'] = medline_rec['JT']\n",
        "    else:\n",
        "        df.loc[j, 'Journal'] = 'null'\n",
        "\n",
        "    if 'AID' in medline_rec:\n",
        "        df.loc[j, 'DOI'] = medline_rec['AID']\n",
        "    else:\n",
        "        df.loc[j, 'DOI'] = 'null'\n",
        "\n",
        "    if 'PMC' in medline_rec:\n",
        "        df.loc[j, 'PMC'] = medline_rec['PMC']\n",
        "    else:\n",
        "        df.loc[j, 'PMC'] = 'null'\n",
        "\n",
        "    mh = \"\"\n",
        "    fau = \"\"\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "ZSoxZzC854Hg"
      },
      "source": [
        "def main():\n",
        "  LEXICON_DIR_PATH = \"SNOMED_CT_and_DataMed_diseases\"   # path to lexicons of ICD-11 diseases - Fixed\n",
        "  icd_chapter_name = \"neoplasms\"   # changes with each name of ICD 11 chapter\n",
        "  OUTPUT_DIR_PATH = \"ICD_11_Final_Corpus\"\n",
        "\n",
        "  os.makedirs(f'{OUTPUT_DIR_PATH}', exist_ok=True)\n",
        "\n",
        "  # call to the query_string generator method defined above\n",
        "  complete_query_terms = _get_search_query_terms(LEXICON_DIR_PATH, icd_chapter_name)   # returns a complete string representation of query terms\n",
        "\n",
        "  df_complete = _query_parse_medline_abstracts(icd_chapter_name, complete_query_terms)   # Call to method to query pubmed for abstracts\n",
        "\n",
        "  df_complete.to_excel(f'{OUTPUT_DIR_PATH}/{icd_chapter_name}_Medline.xlsx')   # write the final corpus to the output directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcMBIwtRWalo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e61caa-5149-4cd3-87ba-b63903cdf154"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FW2Lp-_Xi0Qk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}